# RAG PDF System con FastAPI

Sistema de **Retrieval-Augmented Generation (RAG)** que permite cargar documentos PDF y realizar consultas en lenguaje natural sobre su contenido. El sistema responde exclusivamente basÃ¡ndose en la informaciÃ³n contenida en los documentos, evitando alucinaciones mediante el uso de contexto recuperado.

## ğŸ¯ CaracterÃ­sticas

- **100% Local**: No requiere servicios pagos ni APIs comerciales
- **Sin Alucinaciones**: Responde solo con informaciÃ³n de los documentos cargados
- **Arquitectura Modular**: CÃ³digo limpio y mantenible siguiendo principios SOLID
- **Type-Safe**: ValidaciÃ³n automÃ¡tica con Pydantic
- **Logging Completo**: Trazabilidad de todas las operaciones
- **Persistencia**: Ãndice vectorial guardado en disco
- **API RESTful**: DocumentaciÃ³n automÃ¡tica con Swagger/OpenAPI

## ğŸ—ï¸ Arquitectura

```mermaid
graph TB
    Client[Cliente HTTP] -->|POST /documents/upload| API[FastAPI App]
    Client -->|POST /query| API
    
    API --> PDFService[PDF Service]
    PDFService -->|Extrae texto| Chunking[Chunking Service]
    Chunking -->|Divide en fragmentos| Embedding[Embedding Service]
    Embedding -->|Genera vectores| VectorStore[FAISS Vector Store]
    
    API --> QueryFlow[Query Flow]
    QueryFlow --> Embedding
    Embedding --> VectorStore
    VectorStore -->|Top-K chunks| LLM[LLM Service - Ollama]
    LLM -->|Respuesta contextual| Client
```

### Flujo de Procesamiento

#### 1. Carga de Documentos
1. Usuario sube PDF mediante `/documents/upload`
2. **PDFService** extrae texto completo usando `pdfplumber`
3. **ChunkingService** divide el texto en fragmentos con solapamiento
4. **EmbeddingService** genera embeddings con `sentence-transformers`
5. **VectorStore** almacena vectores en Ã­ndice FAISS
6. Ãndice se guarda en disco para persistencia

#### 2. Consulta de Documentos
1. Usuario envÃ­a pregunta en `/query`
2. **EmbeddingService** genera embedding de la pregunta
3. **VectorStore** busca los K fragmentos mÃ¡s similares
4. Fragmentos se combinan como contexto
5. **LLMService** genera respuesta usando Ollama
6. Si no hay contexto relevante, lo indica explÃ­citamente

## ğŸ› ï¸ Stack TecnolÃ³gico

| Componente | TecnologÃ­a | JustificaciÃ³n |
|------------|------------|---------------|
| **Backend** | FastAPI | Async, validaciÃ³n automÃ¡tica, documentaciÃ³n integrada |
| **PDF Parsing** | pdfplumber | Robusto, maneja tablas y layouts complejos |
| **Embeddings** | sentence-transformers | Modelo open-source ligero (`all-MiniLM-L6-v2`) |
| **Vector Store** | FAISS | Eficiente, local, 100% gratuito |
| **LLM** | Ollama | EjecuciÃ³n local de modelos (Mistral, LLaMA, Phi) |
| **ValidaciÃ³n** | Pydantic | Type-safe, validaciÃ³n automÃ¡tica |
| **Config** | python-dotenv | Variables de entorno |

## ğŸ“‹ Requisitos Previos

- **Python 3.10+**
- **Ollama** instalado y ejecutÃ¡ndose
- ~500MB de espacio en disco (modelos + datos)

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/rag-pdf-system.git
cd rag-pdf-system
```

### 2. Crear entorno virtual

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Instalar y configurar Ollama

#### Windows/Mac
1. Descargar desde [ollama.ai](https://ollama.ai)
2. Instalar y ejecutar Ollama
3. Descargar modelo:

```bash
ollama pull mistral:7b
```

#### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mistral:7b
```

### 5. Configurar variables de entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar .env si es necesario (valores por defecto funcionan)
```

**Variables disponibles:**
- `OLLAMA_BASE_URL`: URL de Ollama (default: `http://localhost:11434`)
- `OLLAMA_MODEL`: Modelo a usar (default: `mistral:7b`)
- `EMBEDDING_MODEL`: Modelo de embeddings (default: `all-MiniLM-L6-v2`)
- `CHUNK_SIZE`: TamaÃ±o de chunks (default: `500`)
- `CHUNK_OVERLAP`: Solapamiento (default: `50`)
- `TOP_K_RESULTS`: Fragmentos a recuperar (default: `3`)

## â–¶ï¸ EjecuciÃ³n

### Iniciar el servidor

```bash
uvicorn app.main:app --reload
```

El servidor estarÃ¡ disponible en `http://localhost:8000`

### DocumentaciÃ³n interactiva

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“¡ Endpoints de la API

### 1. Health Check

**GET** `/health`

Verifica el estado de todos los servicios.

**Respuesta:**
```json
{
  "status": "healthy",
  "ollama_available": true,
  "embedding_model_loaded": true,
  "vector_store_initialized": true
}
```

**Ejemplo con curl:**
```bash
curl http://localhost:8000/health
```

---

### 2. Subir Documento

**POST** `/documents/upload`

Sube y procesa un archivo PDF.

**Request:**
- Content-Type: `multipart/form-data`
- Body: `file` (archivo PDF)

**Respuesta:**
```json
{
  "filename": "documento.pdf",
  "chunks_processed": 42,
  "message": "Document processed successfully"
}
```

**Ejemplo con curl:**
```bash
curl -X POST "http://localhost:8000/documents/upload" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@ruta/al/documento.pdf"
```

**Ejemplo con Python:**
```python
import requests

with open("documento.pdf", "rb") as f:
    response = requests.post(
        "http://localhost:8000/documents/upload",
        files={"file": f}
    )
    
print(response.json())
```

---

### 3. Consultar Documentos

**POST** `/query`

Realiza una pregunta en lenguaje natural sobre los documentos.

**Request:**
```json
{
  "question": "Â¿CuÃ¡l es el tema principal del documento?"
}
```

**Respuesta:**
```json
{
  "answer": "Basado en los documentos, el tema principal es...",
  "sources": [
    {
      "text": "Fragmento relevante del documento...",
      "score": 0.342
    }
  ],
  "has_context": true
}
```

**Ejemplo con curl:**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "Â¿CuÃ¡l es el tema principal?"}'
```

**Ejemplo con Python:**
```python
import requests

response = requests.post(
    "http://localhost:8000/query",
    json={"question": "Â¿CuÃ¡l es el tema principal del documento?"}
)

result = response.json()
print(f"Respuesta: {result['answer']}")
print(f"Fuentes encontradas: {len(result['sources'])}")
```

## ğŸ“ Estructura del Proyecto

```
rag-pdf-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI app y endpoints
â”‚   â”œâ”€â”€ config.py               # ConfiguraciÃ³n con Pydantic
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py          # Modelos Pydantic
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pdf_service.py      # ExtracciÃ³n de texto PDF
â”‚   â”‚   â”œâ”€â”€ chunking_service.py # DivisiÃ³n en fragmentos
â”‚   â”‚   â”œâ”€â”€ embedding_service.py# GeneraciÃ³n de embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py     # GestiÃ³n FAISS
â”‚   â”‚   â””â”€â”€ llm_service.py      # InteracciÃ³n con Ollama
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logger.py           # ConfiguraciÃ³n de logging
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploaded_pdfs/          # PDFs guardados
â”‚   â””â”€â”€ vector_store/           # Ãndice FAISS persistente
â”œâ”€â”€ .env                        # Variables de entorno
â”œâ”€â”€ .env.example               # Plantilla de configuraciÃ³n
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ” Decisiones TÃ©cnicas

### Â¿Por quÃ© RAG?

RAG (Retrieval-Augmented Generation) combina bÃºsqueda de informaciÃ³n con generaciÃ³n de lenguaje:
- **Reduce alucinaciones**: El modelo solo usa informaciÃ³n real
- **Actualizable**: Sin reentrenar, solo aÃ±adir documentos
- **Transparencia**: Se ven las fuentes usadas

### Â¿Por quÃ© FAISS?

- **Local**: No envÃ­a datos a servicios externos
- **Eficiente**: Optimizado por Facebook AI
- **Escalable**: Maneja millones de vectores
- **Gratuito**: 100% open-source

### Â¿Por quÃ© Ollama?

- **Privacidad**: LLM ejecutado localmente
- **Gratuito**: Sin lÃ­mites de API
- **FÃ¡cil**: InstalaciÃ³n simple
- **Flexible**: Soporta mÃºltiples modelos

### Estrategia de Chunking

- **TamaÃ±o**: 500 caracteres (balance contexto/precisiÃ³n)
- **Overlap**: 50 caracteres (preserva contexto entre chunks)
- **Trade-off**: Chunks grandes = mÃ¡s contexto pero menos precisiÃ³n

### Prompt Engineering

El prompt instruye al LLM a:
1. Usar **solo** el contexto proporcionado
2. Indicar cuando no tiene informaciÃ³n
3. No usar conocimiento externo
4. Citar el contexto cuando sea relevante

## âš ï¸ Limitaciones Conocidas

1. **Dependencia de Ollama**: Requiere que Ollama estÃ© ejecutÃ¡ndose
2. **Memoria**: Modelos LLM grandes requieren 8GB+ RAM
3. **Solo PDFs**: No soporta otros formatos (Word, HTML, etc.)
4. **Idioma**: Funciona mejor en inglÃ©s (depende del modelo)
5. **TamaÃ±o de contexto**: Limitado por el modelo LLM usado

## ğŸ§ª Testing

### Test manual bÃ¡sico

1. **Health check**:
```bash
curl http://localhost:8000/health
```

2. **Subir PDF de prueba**:
```bash
curl -X POST "http://localhost:8000/documents/upload" \
  -F "file=@test.pdf"
```

3. **Query con respuesta esperada**:
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "pregunta sobre contenido conocido"}'
```

4. **Query fuera de contexto** (debe indicar que no tiene informaciÃ³n):
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "Â¿QuiÃ©n ganÃ³ el mundial 2022?"}'
```

## ğŸ”§ Troubleshooting

### Error: "Ollama is not available"

**SoluciÃ³n**: Verificar que Ollama estÃ© ejecutÃ¡ndose:
```bash
# Verificar proceso
# Windows: Task Manager
# Linux/Mac:
ps aux | grep ollama

# Reiniciar Ollama
ollama serve
```

### Error: "No text could be extracted from PDF"

**Causas posibles**:
- PDF escaneado (solo imÃ¡genes, sin texto)
- PDF corrupto
- PDF protegido con contraseÃ±a

**SoluciÃ³n**: Usar PDF con texto extraÃ­ble o aplicar OCR previamente

### Error: "Vector store is empty"

**Causa**: No se han subido documentos

**SoluciÃ³n**: Subir al menos un PDF usando `/documents/upload`

### Lentitud en primera ejecuciÃ³n

**Causa**: Primera descarga del modelo de embeddings (~80MB)

**SoluciÃ³n**: Esperar a que se complete la descarga (solo ocurre una vez)

## ğŸ“ˆ PrÃ³ximas Mejoras

- [ ] Soporte para mÃºltiples formatos (DOCX, TXT, HTML)
- [ ] OCR para PDFs escaneados
- [ ] Interfaz web (frontend)
- [ ] AutenticaciÃ³n y multi-usuario
- [ ] MÃ©tricas y analytics
- [ ] Tests unitarios y de integraciÃ³n
- [ ] Docker Compose completo
- [ ] Soporte para chat conversacional (historial)

## ğŸ‘¨â€ğŸ’» Autor

**Lucia** - Desarrollador Python | Backend | Generative AI

- GitHub: [tu-usuario](https://github.com/tu-usuario)
- LinkedIn: [tu-perfil](https://linkedin.com/in/tu-perfil)

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ™ Agradecimientos

- [FastAPI](https://fastapi.tiangolo.com/)
- [FAISS](https://faiss.ai/)
- [Sentence Transformers](https://www.sbert.net/)
- [Ollama](https://ollama.ai/)
- [pdfplumber](https://github.com/jsvine/pdfplumber)

---

Â¿Tienes preguntas o sugerencias? [Abre un issue](https://github.com/tu-usuario/rag-pdf-system/issues) ğŸš€
